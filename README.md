# Image Caption Generation

Welcome! This repository outlines details of the my Computer Vision + Natural Language Processing (NLP) based project - Image Caption Generation.

## Project Summary
This project attempts to replicate and enhance image captioning labels generation accuracy based on existing work published by title: [ClipCap: CLIP Prefix for Image Captioning](https://arxiv.org/abs/2111.09734). In the paper, the approach employs the pre-trained vision-language model CLIP to streamline the captioning process, along with a mapping network for compatibility with the Large Language GPT-2 model.

## Approach and Methodology
In this projects three developments are made:
1. Implementing work in the research paper.
2. Execute experiments to enhance overall performance of the image captioning framework
   1. The first experiment involves integration of the advanced GPT-Neo language model as a substitute for the GPT-2 model.
   2. The second experiment involves usage of sophisticated transformer-based mapping network.
      
## Result
The result shows image caption labels generated through different frameworks.
 
![image](https://github.com/user-attachments/assets/b4dd63ed-0ec0-4f4f-a40c-4739f5274430)

![image](https://github.com/user-attachments/assets/2498cf49-ce2d-4b93-a2a6-9a371319bdeb)

## Tools & Technologies
- **Programming**: Python
- **Frameworks/Libraries**: torchvision, PyTorch
- **Vision Language Models**: CLIP (Contrastive Languageâ€“Image Pre-training)
- **Large Language Models**: GPT-2, GPT-Neo
- **Data Sets**:  COCO
- **Metrics**: COCO, BLEU, METEOR, CIDEr, ROUGE-L
- **IDE**: Google Colab

Are you willing to collaborate in future developments on this project or other related works? If yes, then, feel free to reach out to me at pallaw.kumar15@gmail.com. Please hit like button! :)
